{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference time comparision\n",
    "I was wondering if using hugginface transformers built in model parllelism feature slows down inference. This notebook is used to compare the inference time needed to process the same prompt with an llm utilizing only one or mulitple gpus in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id =  \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "precision = \"16\" # 4, 8, 16, 32 in bits\n",
    "max_tokens = 4096 # max tokens generated by model\n",
    "temperature = 1E-10 # set near 0 to make sure same output is generated in booth runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Count from 1 to 200. DO NOT LEAVE OUT ANY NUMBER.\"}\n",
    "    ] # dummy generation task which takes some time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# measure time with decorator\n",
    "def measure_time(func):\n",
    "    def time_it(*args, **kwargs):\n",
    "        time_started = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        time_elapsed = time.time()\n",
    "        print(\n",
    "            \"{execute} running time is {sec} seconds\"\n",
    "            .format(\n",
    "                execute=func.__name__,\n",
    "                sec=round(time_elapsed - time_started,4) \n",
    "                )\n",
    "            )\n",
    "        return result\n",
    "    return time_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def load_hf_LM_model(model_id, precision, device_map = 'auto'):\n",
    "    if precision == \"32\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map)\n",
    "    elif precision == \"16\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, torch_dtype=torch.float16)\n",
    "    elif precision == \"8\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, load_in_8bit=True)\n",
    "    elif precision == \"4\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, load_in_4bit=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid precision value\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddc2632a35e45b490d3fd00502a3653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_hf_LM_model running time is 4.8644 seconds\n"
     ]
    }
   ],
   "source": [
    "device_map = 'cuda:0' # use one gpu only\n",
    "model = load_hf_LM_model(hf_model_id, precision, device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference running time is 17.2025 seconds\n"
     ]
    }
   ],
   "source": [
    "@measure_time\n",
    "def inference():\n",
    "    return model.generate(encodeds, max_new_tokens=max_tokens, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature = temperature)\n",
    "\n",
    "generated_ids = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "reply = decoded[0].split('[/INST]')[-1].replace('</s>', '')\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0462271c6343be98a2a6a66405b02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_hf_LM_model running time is 4.4114 seconds\n"
     ]
    }
   ],
   "source": [
    "device_map = 'auto' # use device map auto which splits the model to two gpus\n",
    "model = load_hf_LM_model(hf_model_id, precision, device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference running time is 17.5554 seconds\n"
     ]
    }
   ],
   "source": [
    "generated_ids = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "reply = decoded[0].split('[/INST]')[-1].replace('</s>', '')\n",
    "reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference time is nearly the same, seems like model paralellism is implemented quite efficiently at least for this use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-support-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
